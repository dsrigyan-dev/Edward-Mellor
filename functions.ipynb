{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd9da52-6f90-44a4-8cc2-0cd772c157b3",
   "metadata": {},
   "source": [
    "# Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4505f9-1c89-41ac-9528-f2791c98a50c",
   "metadata": {},
   "source": [
    "## Date function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c535e29-0bdf-4178-8143-c85fd88bdd42",
   "metadata": {},
   "source": [
    "### Object to Date Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77dd259-f689-4835-abe6-a80d1afda146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_datetime_or_date(df, columns, format_type='datetime'):\n",
    "    \"\"\"\n",
    "    Convert specified columns in a DataFrame to datetime or date format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns to be converted.\n",
    "    columns (list): List of column names to be converted.\n",
    "    format_type (str): The target format ('datetime' or 'date'). Default is 'datetime'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with specified columns converted.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            if format_type == 'datetime':\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "                # Remove timezone if present to ensure consistency\n",
    "                df[column] = df[column].dt.tz_localize(None)\n",
    "            elif format_type == 'date':\n",
    "                df[column] = pd.to_datetime(df[column]).dt.normalize()\n",
    "                # Remove timezone if present to ensure consistency\n",
    "                df[column] = df[column].dt.tz_localize(None)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid format_type. Use 'datetime' or 'date'.\")\n",
    "        else:\n",
    "            print(f\"Warning: '{column}' not found in DataFrame columns.\")\n",
    "    return df\n",
    "\n",
    "# # example of calling the function\n",
    "# df = convert_to_datetime_or_date(df, ['SalesforceDateTime', 'AnotherDateTime'], format_type='date')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b8510-f8d3-44fb-b8f4-2806782160de",
   "metadata": {},
   "source": [
    "## Access data from salesforce via SOQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b738e69-877a-482f-a07c-a77f0c8d4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce\n",
    "import keyring\n",
    "\n",
    "def export_salesforce_data(query, export_file=None):\n",
    "    # Retrieve your credentials\n",
    "    username = keyring.get_password(\"salesforce\", \"username\")\n",
    "    password = keyring.get_password(\"salesforce\", \"password\")\n",
    "    security_token = keyring.get_password(\"salesforce\", \"security_token\")\n",
    "    consumer_key = keyring.get_password(\"salesforce\", \"consumer_key\")\n",
    "    consumer_secret = keyring.get_password(\"salesforce\", \"consumer_secret\")\n",
    "\n",
    "    # Step 1: Obtain OAuth 2.0 token\n",
    "    token_url = \"https://login.salesforce.com/services/oauth2/token\"\n",
    "    payload = {\n",
    "        'grant_type': 'password',\n",
    "        'client_id': consumer_key,\n",
    "        'client_secret': consumer_secret,\n",
    "        'username': username,\n",
    "        'password': password + security_token\n",
    "    }\n",
    "    response = requests.post(token_url, data=payload)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Extract access token from the response\n",
    "    access_token = response.json().get('access_token')\n",
    "    instance_url = response.json().get('instance_url')\n",
    "\n",
    "    # Step 2: Authenticate to Salesforce using the access token\n",
    "    sf = Salesforce(instance_url=instance_url, session_id=access_token)\n",
    "\n",
    "    # Step 3: Query data\n",
    "    records = []\n",
    "    query_result = sf.query_all(query)\n",
    "    records.extend(query_result['records'])\n",
    "\n",
    "    # Continue querying if there are more records\n",
    "    while not query_result['done']:\n",
    "        query_result = sf.query_more(query_result['nextRecordsUrl'], True)\n",
    "        records.extend(query_result['records'])\n",
    "\n",
    "    # Function to flatten nested dictionaries\n",
    "    def flatten_record(record, parent_key='', sep='.'):\n",
    "        items = []\n",
    "        for k, v in record.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(flatten_record(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    # Flatten all records\n",
    "    flattened_records = [flatten_record(record) for record in records]\n",
    "\n",
    "    # Convert the data to a pandas DataFrame\n",
    "    df = pd.DataFrame(flattened_records)\n",
    "\n",
    "    # Clean up the DataFrame (remove Salesforce metadata)\n",
    "    if 'attributes.type' in df.columns:\n",
    "        df = df.drop(columns=['attributes.type', 'attributes.url'])\n",
    "\n",
    "    # Optionally, export the DataFrame to a CSV file if export_file is provided\n",
    "    if export_file:\n",
    "        df.to_csv(export_file, index=False)\n",
    "        print(f\"Data exported to {export_file}\")\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "# example of calling the function\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# query = \"\"\"\n",
    "# SELECT\n",
    "#     id,\n",
    "#     User__c, \n",
    "#     Name, \n",
    "#     Primary__r.Name,\n",
    "#     Staff_Activated__c,\n",
    "#     Primary__r.POD__r.Name,\n",
    "#     Role_Title__r.Name  \n",
    "# FROM\n",
    "#     Staff__c\n",
    "# \"\"\"\n",
    "# export_file = 'test.csv'\n",
    "\n",
    "# # Get the DataFrame\n",
    "# df = export_salesforce_data(query)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ede409-386b-43fb-9ae8-6aa0953e66fd",
   "metadata": {},
   "source": [
    "## Check duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604fb0e-42d5-42c3-aa84-e681a3e0985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_duplicates(df, columns):\n",
    "    \"\"\"\n",
    "    This function checks for duplicate records based on one or more columns in a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check for duplicates.\n",
    "    columns (str or list): The column name (or a list of column names) to check for duplicates.\n",
    "    \n",
    "    Returns:\n",
    "    bool: Returns True if duplicates are found, otherwise False.\n",
    "    Displays the duplicated rows if any are found.\n",
    "    \"\"\"\n",
    "    # Check if a single column or a list of columns is passed\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Check for duplicates based on the specified column(s)\n",
    "    duplicates = df[df.duplicated(subset=columns, keep=False)]\n",
    "    \n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicates found:\")\n",
    "        print(duplicates)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "# Creating a sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 2, 4],\n",
    "    'B': [5, 6, 6, 8],\n",
    "    'C': ['X', 'Y', 'Y', 'Z']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check for duplicates in a single column\n",
    "check_duplicates(df, 'A')\n",
    "\n",
    "# Check for duplicates in multiple columns\n",
    "check_duplicates(df, ['A', 'B'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad254d-91e9-426b-b5e8-31658aa19a92",
   "metadata": {},
   "source": [
    "## Exporting the data to MYSQL80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f652fe-b697-4c60-8109-7d775481fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "import win32cred\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def get_windows_credentials(target_name):\n",
    "    \"\"\"Retrieve credentials from Windows Credential Manager.\"\"\"\n",
    "    creds = win32cred.CredRead(target_name, win32cred.CRED_TYPE_GENERIC, 0)\n",
    "    username = creds['UserName']\n",
    "    password = creds['CredentialBlob'].decode('utf-16')\n",
    "    return username, password\n",
    "\n",
    "def load_dataframe_to_mysql(df, table_name):\n",
    "    \"\"\"Load a DataFrame into a MySQL table with the same structure as the DataFrame.\"\"\"\n",
    "    # Retrieve credentials\n",
    "    target_name = 'SQLServerConnection'  # The name you used when storing the credentials\n",
    "    username, password = get_windows_credentials(target_name)\n",
    "    \n",
    "    # MySQL connection details\n",
    "    host = 'localhost'  # 'localhost' for local server\n",
    "    database = 'edwardmellorsalesforce'  # Replace with your MySQL database name\n",
    "    \n",
    "    # URL-encode the password\n",
    "    encoded_password = urllib.parse.quote_plus(password)\n",
    "    \n",
    "    # Create a connection string\n",
    "    connection_string = f'mysql+mysqlconnector://{username}:{encoded_password}@{host}/{database}'\n",
    "    \n",
    "    # Create SQLAlchemy engine with increased timeout and autocommit\n",
    "    engine = create_engine(connection_string, connect_args={\"connect_timeout\": 600, \"autocommit\": True})\n",
    "    \n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    try:\n",
    "        # Convert table name to lowercase to avoid case sensitivity issues\n",
    "        table_name = table_name.lower()\n",
    "        \n",
    "        # Debug: Print status before loading the DataFrame\n",
    "        print(f\"Loading DataFrame into MySQL table '{table_name}'...\")\n",
    "        \n",
    "        # Load DataFrame into MySQL with chunking\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "        \n",
    "        # Commit the session\n",
    "        session.commit()\n",
    "\n",
    "        # Debug: Confirm table creation\n",
    "        print(f\"DataFrame successfully exported to MySQL database into table '{table_name}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Rollback in case of error\n",
    "        session.rollback()\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Ensure the connection is properly closed\n",
    "        session.close()\n",
    "        engine.dispose()\n",
    "        print(\"Database connection closed.\")\n",
    "\n",
    "# Example of calling it\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load DataFrame into MySQL\n",
    "#     load_dataframe_to_mysql(Master, 'Master')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
